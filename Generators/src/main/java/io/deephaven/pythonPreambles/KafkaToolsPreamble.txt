# -*-Python-*-
#
# Copyright (c) 2016-2021 Deephaven Data Labs and Patent Pending
#

##############################################################################
#               This code is auto generated. DO NOT EDIT FILE!
# Run generatePythonIntegrationStaticMethods or
# "./gradlew :Generators:generatePythonIntegrationStaticMethods" to generate
##############################################################################


import collections
import sys
import jpy
import wrapt

from ..conversion_utils import _isJavaType, _isStr, _tupleToColDef, _tuplesListToColDefsList

# None until the first _defineSymbols() call
_java_type_ = None
_stream_table_tools = None
SEEK_TO_BEGINNING = None
DONT_SEEK = None

def _defineSymbols():
    """
    Defines appropriate java symbol, which requires that the jvm has been initialized through the :class:`jpy` module,
    for use throughout the module AT RUNTIME. This is versus static definition upon first import, which would lead to an
    exception if the jvm wasn't initialized BEFORE importing the module.
    """

    if not jpy.has_jvm():
        raise SystemError("No java functionality can be used until the JVM has been initialized through the jpy module")

    global _java_type_, _stream_table_tools_
    if _java_type_ is None:
        # This will raise an exception if the desired object is not the classpath
        _java_type_ = jpy.get_type("io.deephaven.kafka.KafkaTools")
        SEEK_TO_BEGINNING = _java_type_.getattr('SEEK_TO_BEGINNING')
        DONT_SEEK = _java_type_.getattr('DONT_SEEK')
        _stream_table_tools_ = jpy.get_type("io.deephaven.db.v2.StreamTableTools")


# every module method should be decorated with @_passThrough
@wrapt.decorator
def _passThrough(wrapped, instance, args, kwargs):
    """
    For decoration of module methods, to define necessary symbols at runtime

    :param wrapped: the method to be decorated
    :param instance: the object to which the wrapped function was bound when it was called
    :param args: the argument list for `wrapped`
    :param kwargs: the keyword argument dictionary for `wrapped`
    :return: the decorated version of the method
    """

    _defineSymbols()
    return wrapped(*args, **kwargs)


@_passThrough
def dictToProperties(dict):
    JProps = jpy.get_type("java.util.Properties")
    r = JProps()
    for key, value in dict.items():
        r.setProperty(key, value)
    return r


@_passThrough
def _custom_avroSchemaToColumnDefinitions(*args):
    if len(args) == 0:
        raise Exception('not enough arguments')
    if len(args) == 1:
        return _java_type_.avroSchemaToColumnDefinitions(args[0])
    if len(args) == 2:
        schema = args[0]
        dict = args[1];
        fieldNamesArray = jpy.array('java.lang.String', dict.keys())
        columnNamesArray = jpy.array('java.lang.String', dict.values())
        mapping = _java_type_.fieldNameMappingFromParallelArrays(fieldNamesArray, columnNamesArray)
        return _java_type_.avroSchemaToColumnDefinitions(schema, mapping)
    raise Exception('too many arguments: ' + len(args))


@_passThrough
def consumeToTable(*args, **kwargs):
    if len(args) != 2:
        raise Exception('not enough positional arguments: expected 2, consumer properties and topic')
    if not isinstance(args[0], dict):
        raise Exception('argument 0 of type dict expected for kafka consumer properties')
    consumer_props_dict = args[0]

    if not _isStr(args[1]):
        raise Exception('argument 1 of type str expected for topic name')
    topic = args[1]

    partitions = kwargs.pop('partitions', None)
    if partitions is None:
        partitions = getattr(_java_type_, 'ALL_PARTITIONS')
    elif isinstance(partitions, str):
        partitions = getattr(_java_type_, partitions)
    elif isinstance(partitions, collections.Sequence):
        try:
            jarr = jpy.array('int', partitionFilter)
        except Exception as e:
            raise Exception(
                "when not of type str, keyword argument 'partitions' has to " +
                "represent a sequence of integer partition values >= 0"
            ) from e
        partitions = _java_type_.partitionFilterFromArray(jarr)
    else
        raise Exception("keyword argument 'partitions' has to be of type str or sequence, instead got partitions=" + str(partitions))

    offsets = kwargs.pop('offsets', None)
    if offsets is None:
        offsets = getattr(_java_type_, 'ALL_PARTITIONS_DONT_SEEK')
    elif isinstance(offsets, str):
        allowed_str_offsets = { "ALL_PARTITIONS_SEEK_TO_BEGINNING", "ALL_PARTITIONS_DONT_SEEK" }
        if offsets not in allowed_str_offsets:
           raise Exception("when of type str, keyword argument 'offsets' has to be one of " + str(allowed_str_offsets))
        offsets = getattr(_java_type_, offsets)
    elif isinstance(offsets, dict):
        try:
            partitionsArray = jpy.array('int', offsets.keys())
            offsetsArray = jpy.array('long', offsets.values())
            partitionToInitialOffset = _java_type_.partitionToOffsetFromParallelArrays(partitionsArray, offsetsArray)
        except Exception as e:
            raise Exception(
                "when of type dict, keyword argument 'offsets' has to map " +
                "numeric partitions to either numeric offsets, or the constants DONT_SEEK and SEEK_TO_BEGINNING, " +
                "instead got offsets=" + str(offsets)
            ) from e
    else:
        raise Exception(
            "type " + type(offsets).__name__ +
            "  of keyword argument 'offsets' not recognized; only str or dict allowed")

    key_avro = kwargs.pop('key_avro', None)
    value_avro = kwargs.pop('value_avro', None)
    key_json = kwargs.pop('key_json', None)
    value_json = kwargs.pop('key_json', None)
    key_simple = kwargs.pop('key', None)
    value_simple = kwargs.pop('value', None)
    key_args_count = reduce(lambda a,b: a+b, map(int, [ key_avro is None, key_json is None, key_simple is None ]))
    value_args_count = reduce(lambda a,b: a+b, map(int, [ value_avro is None, value_json is None, value_simple is None ]))
    if key_args_count == 0 and value_args_count == 0:
        raise Exception('at least one keyword argument for specifying either a key or value is required, none found')
    if key_args_count > 1:
        raise Exception('only one keyword argument for specifying a key is expected, instead ' + key_args_count + 'found')
    if value_args_count > 1:
        raise Exception('only one keyword argument for specifying a value is expected, instead ' + value_args_count + 'found')

    key_spec = None
    if key_avro is not None:
        #
        mapping = getattr(_java_type_, 'DIRECT_MAPPING')
    elif key_json is not None:
        #
        if not isinstance(key_json, list):
            raise Exception('key_json keyword argument should be a list.')
        col_def_list = _tuplesListToColDefsList(key_json)
    elif key_simple is not None:
        #

    value_spec = None
    if value_avro is not None:
        #
        mapping = getattr(_java_type_, 'DIRECT_MAPPING')
    elif value_json is not None:
        #
        if not isinstance(value_json, list):
            raise Exception('value_json keyword argument should be a list.')
        col_def_list = _tuplesListToColDefsList(value_json)
    elif value_simple is not None:
        #

    table_type = kwargs.pop('table_type', None)
    if table_type is None or table_type == 'append':
        return _stream_table_tools_.streamToAppendOnlyTable(streaming_table)
    elif table_type == 'streaming':
        return streaming_table
    raise Exception("unknown value " + table_type + " for keyword argument 'table_type'")


# Define all of our functionality, if currently possible
try:
    _defineSymbols()
except Exception as e:
    pass

