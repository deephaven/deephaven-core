{
 "className": "io.deephaven.base.cache.KeyedObjectCache",
 "methods": {
  "get": "Lookup an item by key.\n\n:param key: (KeyedObjectCache.KEY_TYPE) - The key\n:return: (KeyedObjectCache.VALUE_TYPE) The associated value, or null if none is present.",
  "getCapacity": ":return: int",
  "putIfAbsent": "Add an item to the cache if it's not already present.\n\n:param value: (KeyedObjectCache.VALUE_TYPE) - The value, from which the key will be derived using the key definition.\n:return: (KeyedObjectCache.VALUE_TYPE) The equal (by key) item already present, or value if no such item was found."
 },
 "path": "io.deephaven.base.cache.KeyedObjectCache",
 "text": "The central idea is that we can use an open-addressed map as a bounded cache with concurrent get and synchronized put\n access.\n\n Rather than rely on expensive and/or concurrency-destroying bookkeeping schemes to allow \"smart\" cache replacement,\n we rely on the assumption that our hash function and probe sequence computation does a fairly good job of\n distributing keyed objects that have a high likelihood of being useful to cache during overlapping timeframes.\n\n We never remove anything from the cache without replacing it with a new item. A callback is accepted to allow for\n item resource cleanup upon eviction from the cache.\n\n The impact of collisions (for the bucket an item hashes to, or any other bucket in its associated probe sequence) is\n mitigated by randomized eviction of a victim item in a probe sequence of bounded length.\n\n Note that, unlike common open-addressed hashing schemes, we're unconcerned with load factor - we have an explicitly\n bounded capacity, and explicitly bounded probe sequence length, which must be tuned for the workload in question.",
 "typeName": "class"
}