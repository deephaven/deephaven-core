{
 "className": "io.deephaven.kafka.KafkaTools",
 "methods": {
  "avroSchemaToColumnDefinitions": "**Incompatible overloads text - text from the first overload:**\n\nConvert an Avro schema to a list of column definitions, mapping every avro field to a column of the same name.\n\n*Overload 1*  \n  :param columnsOut: java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>\n  :param fieldPathToColumnNameOut: java.util.Map<java.lang.String,java.lang.String>\n  :param schema: org.apache.avro.Schema\n  :param requestedFieldPathToColumnName: java.util.function.Function<java.lang.String,java.lang.String>\n  \n*Overload 2*  \n  :param columnsOut: (java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>) - Column definitions for output; should be empty on entry.\n  :param schema: (org.apache.avro.Schema) - Avro schema\n  :param requestedFieldPathToColumnName: (java.util.function.Function<java.lang.String,java.lang.String>) - An optional mapping to specify selection and naming of columns from Avro\n          fields, or null for map all fields using field path for column name.\n  \n*Overload 3*  \n  :param columnsOut: (java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>) - Column definitions for output; should be empty on entry.\n  :param schema: (org.apache.avro.Schema) - Avro schema",
  "columnDefinitionsToAvroSchema": ":param t: io.deephaven.engine.table.Table\n:param schemaName: java.lang.String\n:param namespace: java.lang.String\n:param colProps: java.util.Properties\n:param includeOnly: java.util.function.Predicate<java.lang.String>\n:param exclude: java.util.function.Predicate<java.lang.String>\n:param colPropsOut: org.apache.commons.lang3.mutable.MutableObject<java.util.Properties>\n:return: org.apache.avro.Schema",
  "consumeToPartitionedTable": "Consume from Kafka to a Deephaven PartitionedTable containing one constituent Table per\n partition.\n\n:param kafkaProperties: (java.util.Properties) - Properties to configure the result and also to be passed to create the KafkaConsumer\n:param topic: (java.lang.String) - Kafka topic name\n:param partitionFilter: (java.util.function.IntPredicate) - A predicate returning true for the partitions to consume. The convenience constant\n        ALL_PARTITIONS is defined to facilitate requesting all partitions.\n:param partitionToInitialOffset: (java.util.function.IntToLongFunction) - A function specifying the desired initial offset for each partition consumed\n:param keySpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record keys\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record values\n:param tableType: (io.deephaven.kafka.KafkaTools.TableType) - KafkaTools.TableType specifying the type of the expected result's constituent tables\n:return: (io.deephaven.engine.table.PartitionedTable) The result PartitionedTable containing Kafka stream data formatted according to tableType",
  "consumeToResult": "Consume from Kafka to a result Table or PartitionedTable.\n\nNote: Java generics information - <RESULT_TYPE>\n\n:param kafkaProperties: (java.util.Properties) - Properties to configure this table and also to be passed to create the KafkaConsumer\n:param topic: (java.lang.String) - Kafka topic name\n:param partitionFilter: (java.util.function.IntPredicate) - A predicate returning true for the partitions to consume. The convenience constant\n        ALL_PARTITIONS is defined to facilitate requesting all partitions.\n:param partitionToInitialOffset: (java.util.function.IntToLongFunction) - A function specifying the desired initial offset for each partition consumed\n:param keySpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record keys\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record values\n:param tableType: (io.deephaven.kafka.KafkaTools.TableType) - KafkaTools.TableType specifying the type of tables used in the result\n:param resultFactory: io.deephaven.kafka.KafkaTools.ResultFactory<RESULT_TYPE>\n:return: (RESULT_TYPE) The result table containing Kafka stream data formatted according to tableType",
  "consumeToTable": "Consume from Kafka to a Deephaven Table.\n\n:param kafkaProperties: (java.util.Properties) - Properties to configure the result and also to be passed to create the KafkaConsumer\n:param topic: (java.lang.String) - Kafka topic name\n:param partitionFilter: (java.util.function.IntPredicate) - A predicate returning true for the partitions to consume. The convenience constant\n        ALL_PARTITIONS is defined to facilitate requesting all partitions.\n:param partitionToInitialOffset: (java.util.function.IntToLongFunction) - A function specifying the desired initial offset for each partition consumed\n:param keySpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record keys\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record values\n:param tableType: (io.deephaven.kafka.KafkaTools.TableType) - KafkaTools.TableType specifying the type of the expected result\n:return: (io.deephaven.engine.table.Table) The result Table containing Kafka stream data formatted according to tableType",
  "friendlyNameToTableType": "Map \"Python-friendly\" table type name to a KafkaTools.TableType. Supported values are:\n \n* \"stream\"\n* \"append\"\n* \"ring:<capacity>\" where capacity is a integer number specifying the maximum number of trailing rows\n to include in the result\n\n\n:param typeName: (java.lang.String) - The friendly name\n:return: (io.deephaven.kafka.KafkaTools.TableType) The mapped KafkaTools.TableType",
  "getAvroSchema": "Create an Avro schema object for a String containing a JSON encoded Avro schema definition.\n\n:param avroSchemaAsJsonString: (java.lang.String) - The JSON Avro schema definition\n:return: (org.apache.avro.Schema) an Avro schema object",
  "partitionFilterFromArray": ":param partitions: int[]\n:return: java.util.function.IntPredicate",
  "partitionToOffsetFromParallelArrays": ":param partitions: int[]\n:param offsets: long[]\n:return: java.util.function.IntToLongFunction",
  "predicateFromSet": ":param set: java.util.Set<java.lang.String>\n:return: java.util.function.Predicate<java.lang.String>",
  "produceFromTable": "Produce a Kafka stream from a Deephaven table.\n \n Note that table must only change in ways that are meaningful when turned into a stream of events over\n Kafka.\n \n Two primary use cases are considered:\n \n* A stream of changes (puts and removes) to a key-value data set. In order to handle this efficiently\n and allow for correct reconstruction of the state at a consumer, it is assumed that the input data is the result\n of a Deephaven aggregation, e.g. Table.aggAllBy(io.deephaven.api.agg.spec.AggSpec), Table.aggBy(io.deephaven.api.agg.Aggregation), or Table.lastBy(io.deephaven.api.Selectable...). This means\n that key columns (as specified by keySpec) must not be modified, and no rows should be shifted if there\n are any key columns. Note that specifying lastByKeyColumns=true can make it easy to satisfy this\n constraint if the input data is not already aggregated.\n* A stream of independent log records. In this case, the input table should either be a\n stream table or should only ever add rows (regardless of whether the\n attribute is specified).\n\n\n If other use cases are identified, a publication mode or extensible listener framework may be introduced at a\n later date.\n\n:param table: (io.deephaven.engine.table.Table) - The table used as a source of data to be sent to Kafka.\n:param kafkaProperties: (java.util.Properties) - Properties to be passed to create the associated KafkaProducer.\n:param topic: (java.lang.String) - Kafka topic name\n:param keySpec: (io.deephaven.kafka.KafkaTools.Produce.KeyOrValueSpec) - Conversion specification for Kafka record keys from table column data. If not\n        Ignore, must specify a key serializer that maps each input tuple to a unique\n        output key.\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Produce.KeyOrValueSpec) - Conversion specification for Kafka record values from table column data.\n:param lastByKeyColumns: (boolean) - Whether to publish only the last record for each unique key. Ignored when keySpec\n        is IGNORE. Otherwise, if lastByKeycolumns == true this method will internally perform a\n        lastBy aggregation on table grouped by the input columns of\n        keySpec and publish to Kafka from the result.\n:return: (java.lang.Runnable) a callback to stop producing and shut down the associated table listener; note a caller should keep a\n         reference to this return value to ensure liveliness."
 },
 "path": "io.deephaven.kafka.KafkaTools",
 "typeName": "class"
}