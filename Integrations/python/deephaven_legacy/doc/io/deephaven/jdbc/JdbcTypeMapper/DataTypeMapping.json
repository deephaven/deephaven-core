{
 "className": "io.deephaven.jdbc.JdbcTypeMapper$DataTypeMapping",
 "methods": {
  "bindFromChunk": "Bind the given value from the chunk in the given prepared statement.\n\n:param srcChunk: (io.deephaven.chunk.Chunk<io.deephaven.chunk.attributes.Values>) - the chunk to read from\n:param srcOffset: (int) - the location in the chunk to read from\n:param stmt: (java.sql.PreparedStatement) - statement to which to bind the given value\n:param parameterIndex: (int) - parameter index to bind\n:param context: (io.deephaven.jdbc.JdbcTypeMapper.Context) - context information for the binding",
  "bindToChunk": "Get a value from the current row in the given ResultSet, convert to the target type, insert into destination\n chunk. A Context object is provided for additional context or \"settings\" regarding how to perform the\n conversion (for example, the source time zone).\n\n:param destChunk: (io.deephaven.chunk.WritableChunk<io.deephaven.chunk.attributes.Values>) - the chunk to write to\n:param destOffset: (int) - the location in the chunk to write to\n:param resultSet: (java.sql.ResultSet) - from which to extract the value\n:param columnIndex: (int) - ResultSet column from which to extract the value (1-based)\n:param context: (io.deephaven.jdbc.JdbcTypeMapper.Context) - conversion context information",
  "getDeephavenType": ":return: java.lang.Class<JdbcTypeMapper.DataTypeMapping.T>",
  "getInputType": ":return: java.lang.Class<?>"
 },
 "path": "io.deephaven.jdbc.JdbcTypeMapper.DataTypeMapping",
 "text": "An abstraction for mapping a JDBC type to a Deephaven column type. Each implementation of DataTypeMapping\n provides the logic for extracting a value of a specific SQL type from a ResultSet and converting to the Deephaven\n type as well as binding a Deephaven value to a JDBC Statement appropriately (bidirectional mapping).\n\n \n Note that more than one type mapping is possible for each SQL type - a DATE might be converted to either a\n LocalDate or a DateTime for example. And the same is true for the target type - a DateTime might be sourced from\n a SQL DATE or TIMESTAMP value.\n \n\n Most mappings can lose information, making them \"asymmetrical\". A trivial example is integer values - since null\n integers are represented in Deephaven using Integer.MIN_VALUE, an integer value extracted from a JDBC data source\n with this value will appear as NULL in Deephaven. If this value is then written out to a JDBC data source using\n the default mapping behavior, it will be mapped to NULL in JDBC, not the original value. This problem is\n impossible to avoid when the domain of the SQL and Deephaven types do not match precisely.\n \n\n DateTime values have a different problem - SQL and Deephaven DateTime values are conceptually different.\n TIMESTAMP columns in SQL databases typically do not store timezone information. They are equivalent to a java\n LocalDateTime in this respect. However, Deephaven usually stores timestamps in DateTime columns, internally\n represented as nanos-since-the-epoch. This means JDBC timestamps usually require the \"context\" of a timezone in\n order to be converted to a DateTime properly. Writing a DateTime to a JDBC datasource (via the bind mechanism)\n has the same issue. Unfortunately this time-zone context issue applies even when mapping JDBC DATETIME values\n \"directly\" to LocalDate, because most JDBC drivers require all Date related values to pass through a\n java.sql.Date object, which is also epoch-based, not \"local\". The latest JDBC standard deals with this problem\n but doesn't seem to be widely implemented. Here we handle this problem by passing a \"Context\" object every time a\n conversion occurs, which contains the timezone to be used when extracting or binding JDBC date or timestamp\n values.",
 "typeName": "class"
}