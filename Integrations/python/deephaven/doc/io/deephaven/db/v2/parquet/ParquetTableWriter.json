{
 "className": "io.deephaven.db.v2.parquet.ParquetTableWriter",
 "methods": {
  "main": ":param args: java.lang.String[]",
  "write": "Writes a table in parquet format under a given path\n\n*Overload 1*  \n  :param destinationInfos: (io.deephaven.db.v2.parquet.ParquetTableWriter.DestinationInfo[]) - Destination information coupling input data, output path, and grouping metadata for each desired result table\n  :param codecName: (org.apache.parquet.hadoop.metadata.CompressionCodecName) - Compression codec to use\n  :param definition: (io.deephaven.db.tables.TableDefinition) - The definition to use for the output tables\n  :param parallelColumns: (int) - The maximum number of columns that should be written in parallel\n  \n*Overload 2*  \n  :param t: (io.deephaven.db.tables.Table) - The table to write\n  :param path: (java.lang.String) - The destination path\n  :param incomingMeta: (java.util.Map<java.lang.String,java.lang.String>) - A map of metadata values to be stores in the file footer\n  :param groupingPathFactory: (java.util.function.Function<java.lang.String,java.lang.String>) - \n  :param groupingColumns: (java.lang.String...) - List of columns the tables are grouped by (the write operation will store the grouping info)\n  \n*Overload 3*  \n  :param t: io.deephaven.db.tables.Table\n  :param path: java.lang.String\n  :param incomingMeta: java.util.Map<java.lang.String,java.lang.String>\n  :param groupingColumns: java.lang.String...\n  \n*Overload 4*  \n  :param t: (io.deephaven.db.tables.Table) - The table to write\n  :param path: (java.lang.String) - The destination path\n  :param incomingMeta: (java.util.Map<java.lang.String,java.lang.String>) - A map of metadata values to be stores in the file footer\n  :param codecName: (org.apache.parquet.hadoop.metadata.CompressionCodecName) - Compression codec to use\n  :param definition: (io.deephaven.db.tables.TableDefinition) - \n  :param groupingPathFactory: (java.util.function.Function<java.lang.String,java.lang.String>) - \n  :param groupingColumns: (java.lang.String...) - List of columns the tables are grouped by (the write operation will store the grouping info)\n  \n*Overload 5*  \n  :param t: io.deephaven.db.tables.Table\n  :param path: java.lang.String\n  :param incomingMeta: java.util.Map<java.lang.String,java.lang.String>\n  :param codecName: org.apache.parquet.hadoop.metadata.CompressionCodecName\n  :param definition: io.deephaven.db.tables.TableDefinition\n  :param groupingColumns: java.lang.String...\n  \n*Overload 6*  \n  :param t: (io.deephaven.db.tables.Table) - The table to write\n  :param definition: (io.deephaven.db.tables.TableDefinition) - \n  :param path: (java.lang.String) - The destination path\n  :param tableMeta: (java.util.Map<java.lang.String,java.lang.String>) - A map of metadata values to be stores in the file footer\n  :param codecName: (org.apache.parquet.hadoop.metadata.CompressionCodecName) - Name of the codec for compression"
 },
 "path": "io.deephaven.db.v2.parquet.ParquetTableWriter",
 "text": "API for writing DH tables in parquet format",
 "typeName": "class"
}