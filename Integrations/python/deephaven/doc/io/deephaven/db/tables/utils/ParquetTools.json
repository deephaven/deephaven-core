{
 "className": "io.deephaven.db.tables.utils.ParquetTools",
 "methods": {
  "deleteTable": "Deletes a table on disk.\n\n:param path: (java.io.File) - path to delete",
  "readTable": "**Incompatible overloads text - text from the first overload:**\n\nReads in a table from disk, using the provided table definition.\n\n*Overload 1*  \n  :param sourceFilePath: (java.lang.String) - table location; the file should exist and end in \".parquet\" extension.\n  :return: (io.deephaven.db.tables.Table) table\n  \n*Overload 2*  \n  :param sourceFile: (java.io.File) - table location; the file should exist and end in \".parquet\" extension.\n  :return: (io.deephaven.db.tables.Table) table\n  \n*Overload 3*  \n  :param sourceFilePath: (java.lang.String) - table location; the file should exist and end in \".parquet\" extension.\n  :param def: (io.deephaven.db.tables.TableDefinition) - table definition\n  :return: (io.deephaven.db.tables.Table) table\n  \n*Overload 4*  \n  :param sourceFile: (java.io.File) - table location; the file should exist and end in \".parquet\" extension.\n  :param def: (io.deephaven.db.tables.TableDefinition) - table definition\n  :return: (io.deephaven.db.tables.Table) table",
  "setDefaultParquetCompressionCodec": "Sets the default parquet compression codec for writing parquet.\n\n:param codecName: (java.lang.String) - the codec name.",
  "writeParquetTables": "Writes tables to disk in parquet format under a given destinations.  If you specify grouping columns, there\n must already be grouping information for those columns in the sources.  This can be accomplished with\n .by(<grouping columns>).ungroup() or .sort(<grouping column>).\n\n:param sources: (io.deephaven.db.tables.Table[]) - The tables to write\n:param tableDefinition: (io.deephaven.db.tables.TableDefinition) - The common schema for all the tables to write\n:param codecName: (org.apache.parquet.hadoop.metadata.CompressionCodecName) - Compression codec to use.\n:param destinations: (java.io.File[]) - The destinations path\n:param groupingColumns: (java.lang.String[]) - List of columns the tables are grouped by (the write operation will store the grouping info)",
  "writeTable": "Write out a table to disk.\n\n*Overload 1*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param destPath: (java.lang.String) - destination file path; the file name should end in \".parquet\" extension.\n                   If the path includes non-existing directories they are created.\n  \n*Overload 2*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param dest: (java.io.File) - destination file; the file name should end in \".parquet\" extension.\n               If the path includes non-existing directories they are created.\n  \n*Overload 3*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param definition: (io.deephaven.db.tables.TableDefinition) - table definition.  Will be written to disk as given.\n  :param destFile: (java.io.File) - destination file; its path must end in \".parquet\".  Any non existing directories in the path are created.",
  "writeTables": "Write out tables to disk.\n\n:param sources: (io.deephaven.db.tables.Table[]) - source tables\n:param tableDefinition: (io.deephaven.db.tables.TableDefinition) - table definition\n:param destinations: (java.io.File[]) - destinations"
 },
 "path": "io.deephaven.db.tables.utils.ParquetTools",
 "text": "Tools for managing and manipulating tables on disk in parquet format.",
 "typeName": "class"
}