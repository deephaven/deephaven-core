{
 "className": "io.deephaven.engine.table.impl.by.ApproximatePercentile",
 "methods": {
  "accumulateDigests": "Accumulate a Vector of TDigests into a single new TDigest.\n\n \n Accumulate the digests within the Vector into a single TDigest. The compression factor is one third of the\n compression factor of the first digest within the array. If the array has only a single element, then that\n element is returned. If a null array is passed in, null is returned.\n \n\n This function is intended to be used for parallelization. The first step is to independently expose a T-Digest\n aggregation column with the appropriate compression factor on each of a set of sub-tables, using\n Aggregation.AggTDigest(java.lang.String...) and Table.aggBy(io.deephaven.api.agg.Aggregation). Next, call Table.groupBy(String...) to produce\n arrays of Digests for each relevant bucket. Once the arrays are created, use this function to accumulate the\n arrays of digests within an Table.update(java.util.Collection<? extends io.deephaven.api.Selectable>) statement. Finally, you may call the TDigest quantile function\n (or others) to produce the desired approximate percentile.\n \n\n:param array: (io.deephaven.vector.ObjectVector<com.tdunning.math.stats.TDigest>) - an array of TDigests\n:return: (com.tdunning.math.stats.TDigest) the accumulated TDigests",
  "approximatePercentileBy": "Compute the approximate percentiles for the table.\n\n*Overload 1*  \n  :param input: (io.deephaven.engine.table.Table) - the input table\n  :param percentile: (double) - the percentile to compute for each column\n  :return: (io.deephaven.engine.table.Table) a single row table with double columns representing the approximate percentile for each column of the\n           input table\n  \n*Overload 2*  \n  :param input: (io.deephaven.engine.table.Table) - the input table\n  :param percentile: (double) - the percentile to compute for each column\n  :param groupByColumns: (java.lang.String...) - the columns to group by\n  :return: (io.deephaven.engine.table.Table) a with the groupByColumns and double columns representing the approximate percentile for each remaining\n           column of the input table\n  \n*Overload 3*  \n  :param input: (io.deephaven.engine.table.Table) - the input table\n  :param percentile: (double) - the percentile to compute for each column\n  :param groupByColumns: (io.deephaven.engine.table.impl.select.SelectColumn...) - the columns to group by\n  :return: (io.deephaven.engine.table.Table) a with the groupByColumns and double columns representing the approximate percentile for each remaining\n           column of the input table\n  \n*Overload 4*  \n  :param input: (io.deephaven.engine.table.Table) - the input table\n  :param compression: (double) - the t-digest compression parameter\n  :param percentile: (double) - the percentile to compute for each column\n  :param groupByColumns: (io.deephaven.engine.table.impl.select.SelectColumn...) - the columns to group by\n  :return: (io.deephaven.engine.table.Table) a with the groupByColumns and double columns representing the approximate percentile for each remaining\n           column of the input table"
 },
 "path": "io.deephaven.engine.table.impl.by.ApproximatePercentile",
 "text": "Generate approximate percentile aggregations of a table.\n\n \n The underlying data structure and algorithm used is a t-digest as described at https://github.com/tdunning/t-digest,\n which has a \"compression\" parameter that determines the size of the retained values. From the t-digest documentation,\n \"100 is a common value for normal uses. 1000 is extremely large. The number of centroids retained will be a\n smallish (usually less than 10) multiple of this number.&quote;\n \n\n All input columns are cast to doubles and the result columns are doubles.\n \n\n The input table must be add only, if modifications or removals take place; then an UnsupportedOperationException is\n thrown. For tables with adds and removals you must use exact percentiles with Aggregation.AggPct(double, java.lang.String...).\n \n\n You may compute either one approximate percentile or several approximate percentiles at once. For example, to compute\n the 95th percentile of all other columns, by the \"Sym\" column you would call:\n \n \n ApproximatePercentile.approximatePercentile(input, 0.95, \"Sym\")\n \n\n If you need to compute several percentiles, it is more efficient to compute them simultaneously. For example, this\n example computes the 75th, 95th, and 99th percentiles of the \"Latency\" column using a builder pattern, and the 95th\n and 99th percentiles of the \"Size\" column by \"Sym\":\n \n \n final Table aggregated = input.aggBy(List.of(\n         Aggregation.ApproxPct(\"Latency', PctOut(0.75, \"L75\"), PctOut(0.95, \"L95\"), PctOut(0.99, \"L99\")\n         Aggregation.ApproxPct(\"Size', PctOut(0.95, \"S95\"), PctOut(0.99, \"S99\")));\n \n\n When parallelizing a workload, you may want to divide it based on natural partitioning and then compute an overall\n percentile. In these cases, you should use the Aggregation.AggTDigest(java.lang.String...) aggregation to expose the internal\n t-digest structure as a column. If you then perform an array aggregation (Table.groupBy(java.util.Collection<? extends io.deephaven.api.Selectable>)), you can call the\n accumulateDigests(io.deephaven.vector.ObjectVector<com.tdunning.math.stats.TDigest>) function to produce a single digest that represents all of the constituent digests. The\n amount of error introduced is related to the compression factor that you have selected for the digests. Once you have\n a combined digest object, you can call the quantile or other functions to extract the desired percentile.",
 "typeName": "class"
}