{
 "className": "io.deephaven.kafka.KafkaTools",
 "methods": {
  "avroSchemaToColumnDefinitions": "**Incompatible overloads text - text from the first overload:**\n\nConvert an Avro schema to a list of column definitions, mapping every avro field to a column of the same name.\n\n*Overload 1*  \n  :param columns: java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>\n  :param mappedOut: java.util.Map<java.lang.String,java.lang.String>\n  :param schema: org.apache.avro.Schema\n  :param fieldNameToColumnName: java.util.function.Function<java.lang.String,java.lang.String>\n  \n*Overload 2*  \n  :param columns: (java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>) - Column definitions for output; should be empty on entry.\n  :param schema: (org.apache.avro.Schema) - Avro schema\n  :param fieldNameToColumnName: (java.util.function.Function<java.lang.String,java.lang.String>) - An optional mapping to specify selection and naming of columns from Avro fields, or\n          null for map all fields using field name for column name.\n  \n*Overload 3*  \n  :param columns: (java.util.List<io.deephaven.engine.table.ColumnDefinition<?>>) - Column definitions for output; should be empty on entry.\n  :param schema: (org.apache.avro.Schema) - Avro schema",
  "consumeToTable": "Consume from Kafka to a Deephaven table.\n\n:param kafkaProperties: (java.util.Properties) - Properties to configure this table and also to be passed to create the KafkaConsumer\n:param topic: (java.lang.String) - Kafka topic name\n:param partitionFilter: (java.util.function.IntPredicate) - A predicate returning true for the partitions to consume\n:param partitionToInitialOffset: (java.util.function.IntToLongFunction) - A function specifying the desired initial offset for each partition consumed\n:param keySpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record keys\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Consume.KeyOrValueSpec) - Conversion specification for Kafka record values\n:param resultType: (io.deephaven.kafka.KafkaTools.TableType) - KafkaTools.TableType specifying the type of the expected result\n:return: (io.deephaven.engine.table.Table) The result table containing Kafka stream data formatted according to resultType",
  "friendlyNameToTableType": "Map \"Python-friendly\" table type name to a KafkaTools.TableType.\n\n:param typeName: (java.lang.String) - The friendly name\n:return: (io.deephaven.kafka.KafkaTools.TableType) The mapped KafkaTools.TableType",
  "getAvroSchema": "**Incompatible overloads text - text from the first overload:**\n\nFetch an Avro schema from a Confluent compatible Schema Server.\n\n*Overload 1*  \n  :param schemaServerUrl: (java.lang.String) - The schema server URL\n  :param resourceName: (java.lang.String) - The resource name that the schema is known as in the schema server\n  :param version: (java.lang.String) - The version to fetch, or the string \"latest\" for the latest version.\n  :return: (org.apache.avro.Schema) An Avro schema.\n  \n*Overload 2*  \n  :param schemaServerUrl: (java.lang.String) - The schema server URL\n  :param resourceName: (java.lang.String) - The resource name that the schema is known as in the schema server\n  :return: (org.apache.avro.Schema) An Avro schema.",
  "partitionFilterFromArray": ":param partitions: int[]\n:return: java.util.function.IntPredicate",
  "partitionToOffsetFromParallelArrays": ":param partitions: int[]\n:param offsets: long[]\n:return: java.util.function.IntToLongFunction",
  "produceFromTable": "Consume from Kafka to a Deephaven table.\n\n:param table: (io.deephaven.engine.table.Table) - The table used as a source of data to be sent to Kafka.\n:param kafkaProperties: (java.util.Properties) - Properties to be passed to create the associated KafkaProducer.\n:param topic: (java.lang.String) - Kafka topic name\n:param keySpec: (io.deephaven.kafka.KafkaTools.Produce.KeyOrValueSpec) - Conversion specification for Kafka record keys from table column data.\n:param valueSpec: (io.deephaven.kafka.KafkaTools.Produce.KeyOrValueSpec) - Conversion specification for Kafka record values from table column data.\n:param lastByKeyColumns: (boolean) - Whether to publish only the last record for each unique key. Ignored when keySpec\n        is IGNORE. If keySpec != null && !lastByKeyColumns, it is expected that table will\n        not produce any row shifts; that is, the publisher expects keyed tables to be streams, add-only, or\n        aggregated.\n:return: (java.lang.Runnable) a callback to stop producing and shut down the associated table listener; note a caller should keep a\n         reference to this return value to ensure liveliness."
 },
 "path": "io.deephaven.kafka.KafkaTools",
 "typeName": "class"
}