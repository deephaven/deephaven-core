{
 "className": "io.deephaven.csv.densestorage.DenseStorageWriter",
 "methods": {
  "append": "Append a ByteSlice to the queue. The data will be diverted to one of the two specialized underlying\n queues, depending on its size.\n\n:param bs: io.deephaven.csv.containers.ByteSlice",
  "finish": "Call this method to indicate when you are finished writing to the queue.",
  "newReader": ":return: io.deephaven.csv.densestorage.DenseStorageReader"
 },
 "path": "io.deephaven.csv.densestorage.DenseStorageWriter",
 "text": "The DenseStorageWriter and DenseStorageReader work in tandem, forming a FIFO queue. The DenseStorageWriter\n writes data, and the DenseStorageReader reads that data. If the DenseStorageReader \"catches up\", it\n will block until the DenseStorageWriter provides more data, or indicates that it is done (via the finish()\n method. This synchronization is done at \"block\" granularity, so the DenseStorageReader can only proceed when the\n DenseStorageWriter has written at least a \"block\" of data or is done. We allow multiple independent\n DenseStorageReaders to consume the same underlying data. In our implementation this is used so our type\n inferencer can take a second \"pass\" over the same input data.\n\n \n The point of this object is to store a sequence of (character sequences aka \"strings\", but not java.lang.String),\n using a small fraction of overhead. The problem with storing every character sequence as a java.lang.String is:\n \n* Per-object overhead (probably 8 or 16 bytes depending on pointer width)\n* The memory cost of holding a reference to that String (again 4 or 8 bytes)\n* The string has to know its length (4 bytes)\n* Java characters are 2 bytes even though in practice many strings are ASCII-only and their chars can fit in a\n byte. (Newer Java implementations can store text as bytes, eliminating this objection)\n\n\n For small strings (say the word \"hello\" or the input text \"12345.6789\") the overhead can be 100% or worse.\n\n For our purposes we:\n \n* Only need sequential access. i.e. we don't need random access into the sequence of \"strings\". So we can support a\n model where we can have a forward-only cursor moving over the sequence of \"strings\".\n* Don't need to give our caller a data structure that they can hold on to. The caller only gets a \"view\" (a slice)\n of the current \"string\" data. The view is invalidated when they move to the next \"string\"\n\n\n Furthermore we:\n \n* Offer a FIFO model where the reader (in a separate thread) can chase the writer but there is not an inordinate\n amount of synchronization overhead (synchronization happens at the block level, not the \"string\" level).\n* Have the ability to make multiple Readers which pass over the same underlying data. This is our low-drama way of\n allowing our client to make multiple passes over the data, without complicating the iteration interface, with, e.g.,\n a reset method.\n* Use a linked-list structure so that when all existing readers have move passed a block of data, that block can be\n freed by the garbage collector without any explicit action taken by the reader.\n\n\n If you are familiar with the structure of our inference, you may initially think that this reader-chasing-writer\n garbage collection trick doesn't buy us much because we have a two-phase parser. However, when the inferencer has\n gotten to the last parser in its set of allowable parsers (say, the String parser), or the user has specified that\n there is only one parser for this column, then the code doesn't need to do any inference and can parse the column in\n one pass. In this case, when the reader stays caught up with the writer, we are basically just buffering one block of\n data, not the whole file.\n\n \n The implementation used here is to look at the \"string\" being added to the writer and categorize it along two\n dimensions:\n \n* Small vs large\n* Byte vs char\n\n\n These dimensions are broken out in the following way:\n * Small byte \"strings\" are packed into a byte block, and we maintain a linked list of these byte blocks.\n* \"Large\" byte \"strings\" are stored directly, meaning a byte[] array is allocated for their data, then a reference\n to that array is added to a byte-array block. (And again, we maintain a linked list of these byte-array blocks). It\n is not typical for CSV data to contain a cell this large, but the feature is there for completeness. We do not want\n want large \"strings\" to contaminate our packed byte blocks because they would not likely pack into them tightly (it\n would become more likely to have allocated blocks with unused storage at the end, because the last big string\n wouldn't fit in the current block). It's OK to keep them on their own because by definition, large \"strings\" are not\n going to have much overhead, as a percentage of the size of their text content.",
 "typeName": "class"
}